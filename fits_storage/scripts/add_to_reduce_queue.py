#! /usr/bin/env python3

import datetime
import os.path
import json

from sqlalchemy.exc import NoResultFound, MultipleResultsFound

from fits_storage.config import get_config
fsc = get_config()

from fits_storage.logger import logger, setdebug, setdemon, setlogfilesuffix

from fits_storage.db import session_scope
from fits_storage.core.orm.diskfile import DiskFile
from fits_storage.core.orm.header import Header
from fits_storage.queues.queue.reducequeue import ReduceQueue, memory_estimate
from fits_storage.queues.orm.reducequeentry import debundle_options

from fits_storage.db.list_headers import list_headers
from fits_storage.db.selection.get_selection import from_url_things

from fits_storage.server.reduce_list import parse_listfile


if __name__ == "__main__":
    # Option Parsing
    from argparse import ArgumentParser
    # ------------------------------------------------------------------------------
    parser = ArgumentParser()

    parser.add_argument("--debug", action="store_true", dest="debug",
                        help="Increase log level to debug")

    parser.add_argument("--demon", action="store_true", dest="demon",
                        help="Run in the background, do not generate stdout")

    parser.add_argument("--filenames", action="extend", type=str,
                        dest="filenames", default=[], nargs='+',
                        help="Add this space separated list of filenames as a "
                             "single entry to the queue")

    parser.add_argument("--listfile", action="store", type=str, default=None,
                        help="Filename of a file containing one or more lists "
                             "of files to add to the queue, each list as a "
                             "single entry. One list per line in the file.")

    parser.add_argument("--selection", action="store", type=str, default=None,
                        help="URL-style selection criteria. Add files matching"
                             "this selection in the database, as individual"
                             "file entries to the reduce queue.")

    parser.add_argument("--initiatedby", action="store", type=str, default=None,
                        help="Processing Initiated By record for reduced data."
                             "Cannot be defaulted in production environments")

    parser.add_argument("--intent", action="store", type=str, default=None,
                        help="Processing Intent record for reduced data. "
                             "Science-Quality or Quick-Look. Can use sq or ql."
                             "Cannot be defaulted in production environments")

    parser.add_argument("--tag", action="store", type=str, default=None,
                        help="Processing Tag record for reduced data."
                             "Cannot be defaulted in production environments")

    parser.add_argument("--recipe", action="store", type=str, default=None,
                        help="DRAGONS recipe name to use. Omit to use"
                             "DRAGONS default")

    parser.add_argument("--debundle", action="store", type=str, default=None,
                        help="Debundle strategy, omit for None - ie no debundling")

    parser.add_argument("--uparms", action="store", type=str, default=None,
                        help="uparms to pass to Reduce. Format is a string "
                             "representation of a python dictionary")

    parser.add_argument("--capture_files", action="store_true",
                        help="Capture reduced files output from this processing"
                             " run? Default is False")

    parser.add_argument("--capture_monitoring", action="store_true",
                        help="Capture monitoring values generated by this "
                             "processing run? Default is False")

    parser.add_argument("--batch", action="store",
                        help="Batch name for this reduction. See --after_batch")

    parser.add_argument("--after_batch", action="store",
                        help="Only process this reduction entry when there are "
                             "no entries with this batch name on the reduce, "
                             "fileops or ingest queues. This can be used to "
                             "ensure that calibrations are reduced, transferred"
                             ", and ingested before processing science files "
                             "that need those calibrations")

    parser.add_argument("--json_write", action="store",
                        help="File to record entries to. The file must be empty"
                             " or be a JSON list. Additional entries will be "
                             "appended to the list.")

    parser.add_argument("--json_read", action="store",
                        help="JSON file to read entries from.")

    parser.add_argument("--dryrun", action="store_true",
                        help="Do not actually add to database. Does not prevent"
                             " reading and writing JSON files")
    parser.add_argument("--logsuffix", action="store", type=str,
                        dest="logsuffix", default=None,
                        help="Extra suffix to add on logfile")

    options = parser.parse_args()

    # Logging level to debug? Include stdio log?
    setdebug(options.debug)
    setdemon(options.demon)

    # Check Log Suffix
    if options.logsuffix:
        setlogfilesuffix(options.logsuffix)

    # Announce startup
    logger.info("*** add_to_reduce_queue.py - starting up at {}"
                .format(datetime.datetime.now()))
    logger.debug("Config files used: %s", ', '.join(fsc.configfiles_used))

    if options.debundle and options.debundle not in debundle_options:
        logger.error("Invalid debundle option: %s", options.debundle)
        exit(1)

    initiatedby = options.initiatedby
    intent = options.intent
    tag = options.tag
    # Check for default processing records in production servers
    if fsc.fits_system_status == 'development':
        if initiatedby is None:
            logger.warning("No Processing Initiated By specified. "
                           "Setting to DEVELOPER")
            initiatedby = 'DEVELOPER'
        if intent is None:
            logger.warning("No Processing Intent specified. "
                           "Setting to Quick-Look")
            intent = 'Quick-Look'
        if tag is None:
            logger.warning("No Processing Tag specified."
                           "Setting to TEST")
            tag = 'TEST'
    else:
        # Not a development server
        if None in (initiatedby, intent, tag):
            logger.error("Required Processing Record not specified, aborting")
            exit(1)

    if options.filenames:
        # Just add a list of filename
        logger.info("Adding single entry list of filenames: %s",
                    options.filenames)
        files = options.filenames
        lists = [files]

    elif options.listfile:
        # Get list(s) of files from list file
        logger.info("Adding files from list file: %s" % options.listfile)
        with open(options.listfile) as fp:
            lists = parse_listfile(fp)

    elif options.selection:
        # Get list from database
        things = options.selection.split('/')
        selection = from_url_things(things)
        logger.info("Selection: %s" % selection)
        if selection.openquery:
            logger.warning("Selection is open - this may not be what you want")

    else:
        logger.info("No list(s) of filenames was provided.")
        lists = []

    # Keep a list of rqes we add, as dictionaries, so that we can write them
    # to json at the end if requested. We store them as dicts rather than the
    # actual rqes so that there is no session dependence.
    rqeds = []

    with session_scope() as session:

        if options.selection:
            logger.info("Getting header object list")
            headers = list_headers(selection, [], session=session,
                                   unlimit=True)

            # Looping through the header list directly for the add
            # is really slow if the list is big.
            logger.info("Building filename lists")
            lists = []
            for header in headers:
                lists.append([header.diskfile.filename])
            headers = None
            logger.info(f"Selection found {len(lists)} files to add")

        for filelist in lists:
            # Check that all the filenames given are valid and ensure they end
            # in .fits
            validfiles = []
            numpix = []
            for filename in filelist:
                if filename.endswith('.fits.bz2'):
                    filename = filename.removesuffix('.bz2')
                elif filename.endswith('.fits'):
                    pass
                else:
                    filename += '.fits'

                possible_filenames = [filename, filename+'.bz2']

                query = session.query(Header).join(DiskFile) \
                    .filter(DiskFile.present == True)\
                    .filter(DiskFile.filename.in_(possible_filenames))

                try:
                    header = query.one()
                except NoResultFound:
                    logger.error("Filename %s not found in database, not adding "
                                 "this file to the list", filename)
                except MultipleResultsFound:
                    logger.error("Filename %s has multiple results, not adding "
                                 "this file to the list", filename)
                else:
                    validfiles.append(filename)
                    # Deprecate header.estimate_numpix() once the header numpix
                    # column is fully populated after database rebuilds.
                    tmp_numpix = header.numpix if header.numpix is not None else header.estimate_numpix()

                    # Put in a correction for GHOST here - we only ever process
                    # half the GHOST pixels at once
                    if header.instrument == 'GHOST':
                        tmp_numpix //= 2
                    numpix.append(tmp_numpix)

            logger.debug(f"List of validated files: {validfiles}, {numpix=}")

            if len(validfiles):
                rq = ReduceQueue(session, logger=logger)
                if options.dryrun:
                    logger.info(f"Dryrun - not actually Queuing a batch of "
                                f"{len(validfiles)} files for "
                                f"reduce, starting with {validfiles[0]}")
                else:
                    logger.info(f"Queuing a batch of {len(validfiles)} files for "
                                f"reduce, starting with {validfiles[0]}")
                rqe = rq.add(validfiles,
                             intent=intent,
                             initiatedby=initiatedby,
                             tag=tag,
                             recipe=options.recipe, uparms=options.uparms,
                             capture_files=options.capture_files,
                             capture_monitoring=options.capture_monitoring,
                             batch=options.batch,
                             after_batch=options.after_batch,
                             debundle=options.debundle,
                             mem_gb=memory_estimate(numpix),
                             dryrun=options.dryrun)
                rqeds.append(rqe.as_dict())
            else:
                logger.error("No valid files to add")

        if options.json_read:
            logger.info(f"Reading from JSON file: {options.json_read}")
            try:
                with open(options.json_read, 'r') as jf:
                    entries = json.load(jf)
                logger.info(f"Read {len(entries)} entries from JSON")
            except Exception:
                logger.error("Failed to read entries from JSON", exc_info=True)
                entries = []

            # Are we over-riding anything (e.g. processing tag etc.) from the
            # command line?
            overrides = {}
            for item in ('initiatedby', 'intent', 'tag', 'recipe', 'debundle',
                         'uparms', 'capture_monitoring', 'capture_files',
                         'batch', 'after_batch'):
                if getattr(options, item):
                    logger.debug(f"Over-riding {item} from command line: "
                                 f"{getattr(options, item)}")
                    overrides[item] = getattr(options, item)

            rq = ReduceQueue(session, logger=logger)
            for entry in entries:
                entry |= overrides
                if options.dryrun:
                    logger.info(f"Dryrun - not actually adding an entry of {len(entry['filenames'])} files "
                                f"for reduce, starting with {entry['filenames'][0]}")
                else:
                    logger.info(f"Adding an entry of {len(entry['filenames'])} files "
                                f"for reduce, starting with {entry['filenames'][0]}")
                # Add the rqe. Commit all the entries in a single transaction,
                # so that if one fails they all fail, and also to ensure that
                # batch and after_batch dependencies are respected
                rqe = rq.add(commit=False, dryrun=options.dryrun, **entry)

            # Commit all the entries in one transaction
            if not options.dryrun:
                try:
                    session.commit()
                    logger.info("Transaction committed. All items added sucessfully")
                except Exception:
                    logger.error("Failed to commit the transaction adding all the "
                                 "entries. None have been added.", exc_info=True)


    if options.json_write:
        jfn = options.json_write
        logger.info(f"Writing Reduce Queue Entries to json file {jfn}")
        if os.path.exists(jfn):
            # read existing entries
            with open(jfn, 'r') as jf:
                entries = json.load(jf)
                logger.debug(f"Read {len(entries)} existing entries from json")
            entries += rqeds
            logger.debug(f"Appended {len(rqeds)} new entries to list")
        else:
            entries = rqeds

        # Write the json file, overwriting if it already exists
        with open(jfn, 'w') as jf:
            logger.debug(f"Writing {len(entries)} to json file")
            json.dump(entries, jf, indent=2)

    logger.info("*** add_to_reducequeue.py exiting normally at %s",
                datetime.datetime.now())
